---
title: 'Project 1: Exploratory Data Analysis'
author: "SDS348 Fall 2019"
date: "9/16/2019"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, message=FALSE, warning=FALSE, fig.width=8, tidy=TRUE)
```

## Data Wrangling and Data Exploration

### Instructions
A knitted R Markdown document (as a PDF) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on 10/20/2019. These two documents will be graded jointly, so they must be consistent (i.e., donâ€™t change the R Markdown file without also updating the knitted document). Knit an html copy too, for later!

I envision your written text forming something of a narrative structure around your code/output. All results presented must have corresponding code. Any answers/results/plots etc. given without the corresponding R code that generated the result will not be graded. Furthermore, all code contained in your final project document should work properly. Please do not include any extraneous code or code which produces error messages. (Code which produces warnings is acceptable, as long as you understand what the warnings mean).

### Find data:

Find two (!) datasets with one variable in common (e.g., dates, years, states, counties, countries), both with at least 50 observations (i.e., rows) in each. When combined, the resulting/final dataset must have **at least 4 different variables (at least 2 numeric) in addition to the common variable**.

Choose as many as you would like! If you found two datasets that you like but they don't have enough variables,  find a third dataset with the same common variable and join all three.


### Guidelines

1. If the datasets are not tidy, you will need to reshape them so that every observation has its own row and every variable its own column. If the datasets are both already tidy, you will make them untidy with pivot_wider()/spread() and then tidy them again with pivot_longer/gather() to demonstrate your use of the functions. It's fine to wait until you have your descriptives to use these functions (e.g., you might want to pivot_wider() to rearrange the data to make your descriptive statistics easier to look at); it's fine long as you use them at least once!

    - Depending on your datasets, it might be a good idea to do this before joining. For example, if you have a dataset you like with multiple measurements per year, but you want to join by year, you could average over your numeric variables to get means/year, do counts for your categoricals to get a counts/year, etc.
    
    I chose two datasets which contain water quality and flow data information collecting from the same IBWC Gage at the Rio Grande Below Amistad Dam near Del Rio, TX. The first dataset, RioBelowDamB, contains the Monthly Flow Averages (ac-ft/mo) from 1956 to 2005. The second dataset, RioC, contains 40 different variables, like Dissolved Sodium (mg/L), Instantaneous Stream Flow (CFS), and Water Temperature (Centigrade), from the years 1972-1985. I currently am interning at a nonprofit called Devils River Conservancy (a tributary of the Rio Grande and one of the most pristine rivers in Texas), where I am constructing a database of all relevant scientific, cultural and anthropological information concerning the Devils River. These two datasets were both sent to me to include in the database and were collected by the International Boundary & Water Commission and the Texas Water Commission. These datasets are interesting to me, not only because I want to pursue a career in helping manage and conserve Texas' water resources, but also because the Rio Grande is considered one of the most endangered river systems in North America and I am curious to see if any patterns become relevant as I work with the data. I'm sure there will be a correlation between Monthly Average Stream Flow and Instantaneous Stream Flow, but I am curious as to whether there will be a correlation between Monthly Average Stream Flow and some of the measured dissolved chemicals in the river. I am also curious as to whether the pH changes due to temperature, or dissolved chemicals, and if there is a consistant correlation between Stream Flow and the Months.
    
```{r}
library(dplyr)
library(tidyr)
library(tibble)
library(ggplot2)
library(scales)
library(ggrepel)
riobelowdamneardelrio <- read.csv("~/riobelowdamneardelrio.csv")
RioC <- read.csv("~/RioC.csv")
RioBelowDamB<- riobelowdamneardelrio
RioB<- RioBelowDamB %>% pivot_longer(c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"), names_to = "Month", values_to = "MonthlyFlowAvg")
RioC3<- RioC %>% group_by(`Primary.Station.ID`, `Start.Date`, `Parameter.Long.Name`)  %>% mutate(rn = row_number()) %>%
  pivot_wider(names_from = "Parameter.Long.Name", values_from = "Result.Value")
RioC3<- RioC3 %>% select(-rn)
RioC32<- RioC3 %>% select(-`Parameter.Code`)
RioC33<- RioC32 %>% select(-`Organization.Name`, -`Station.Location.Name`, -
State, -County, -Latitude, -Longitude, -`Hydrologic.Unit.Code`, -`Legacy.STORET.Station.Type.Code`, -`Surface.Water.Indicator`, -`Composite.Method.Code`, -`Composite.Statistic.Code`, -`Sample.Depth`)
RioC33<- RioC33 %>% select(-`Organization.Code`, -`Primary.Station.ID`, -`Secondary.ID..1`, -`Secondary.ID..2`, -`Secondary.ID..3`)
   RioC33<- RioC33 %>% select(-`End.Time`, -`CODE NO FOR AGENCY ANALYZING SAMPLE (SEE APPEND)`, -`CODE NO FOR AGENCY COLLECTING SAMPLE-SEE APPEND.`)
RioC33<- RioC33 %>% ungroup("Start.Date")
Rio<- RioC33 %>% separate(Start.Date, into = c("Year", "Month", "Day"), sep= "-", convert=T)
RioC44 <- Rio %>% filter(!is.na("Sample.Code"))
Rio1<- RioC44 %>%
  group_by(`Month`, `Start.Time`) %>%
  summarise_each(funs(first(.[!is.na(.)])))
 RioC33<- Rio1 %>% select( -`Sample.Code`, -`X`, -Primary.Station.ID, -Station.Location.Name.2, -Station.Location.Name.3, -Ground.Water.Indicator, -Pipe.Indicator,    -End.Date, -UMK, -Composite.Grab.Number, -Efluent.Monitoring.Code, -Replicate.Number, -Pipe.ID, -Primary.Activity.Category, -Secondary.Activity.Category)
Rio9<- transform(RioC33, Month = month.abb[Month])
Rio1C<- Rio9 %>% select(-Remark.Code)
```
    
    In order to tidy RioBelowDamB I used pivot_longer to move the Months from column names to their own column entitled "Months". I created a new column for Total Average Stream Flow as well, and moved the remaining values to their own column entitled MonthlyFlowAvg. For RioC I removed several columns that were either empty or contained categorical variables that were unnecessary for my desired results. I used pivot_wider to move the variables from the column "Parameter Long Name" and gave each variable it's own column with its listed values, from "ResultValue". I separated the column "Start Date" into Month, Day, and Year, respectively, and changed the Month values from numbers to month abbreviations. 

2. Join your 2+ separate data sources into a single dataset

    - You will document the type of join that you do (left/right/inner/full), including how many cases in each dataset were dropped and why you chose this particular join
```{r}
Rio2C<- Rio1C
Rio2C$MY<- paste(Rio2C$Month, "-", Rio2C$Year)
Rio2B<- RioB
Rio2B$MY<- paste(Rio2B$Month, "-", Rio2B$Year)
RioBC<- inner_join(Rio2B, Rio2C, by = "MY")
RioBC<- RioBC %>% select(-Year.y, -Month.y)
RioBC<- rename(RioBC, Year = Year.x)
RioBC<- rename(RioBC, Month = Month.x)
RioBCU<- RioBC %>% select(-TEMPERATURE..WATER..DEGREES.FAHRENHEIT. ,-TURBIDITY...JACKSON.CANDLE.UNITS., 
 -BOD..5.DAY..20.DEG.C..........................MG.L,  -OXYGEN..DISSOLVED..PERCENT.OF.SATURATION..........,  
 -PH..LAB..STANDARD.UNITS.........................SU, -SPECIFIC.CONDUCTANCE.FIELD..UMHOS.CM...25C.,
 -CHLORIDE..DISSOLVED.IN.WATER...........MG.L,-COLIFORM.TOT.MPN.CONFIRMED.TEST.35C..TUBE.31506., 
 -RESIDUE..TOTAL.NONFILTRABLE..MG.L., -NITROGEN..AMMONIA..TOTAL..MG.L.AS.N., 
 -AMMONIA..UNIONIZED..CALC.FR.TEMP.PH.NH4....MG.L., -AMMONIA..UNIONZED.......................MG.L.AS.N., 
 -NITRATE.NITROGEN..TOTAL..MG.L.AS.N., -PHOSPHATE..TOTAL..MG.L.AS.PO4., -PHOSPHORUS..TOTAL..MG.L.AS.P.,
 -CHLOROPHYLL.A.UG.L.SPECTROPHOTOMETRIC.ACID..METH.,  -RESIDUE..VOLATILE.NONFILTRABLE..MG.L.,
 -RESIDUE.TOTAL.FILTRABLE..DRIED.AT.180C..MG.L, -HARDNESS..TOTAL..MG.L.AS.CACO3., -SODIUM.ADSORPTION.RATIO,  
 -SOLIDS..DISSOLVED.SUM.OF.CONSTITUENTS..MG.L., -PHOSPHORUS..DISSOLVED.ORTHOPHOSPHATE..MG.L.AS.P.,
 -CARBON..TOTAL.ORGANIC..MG.L.AS.C., -PHEOPHYTIN.A.UG.L.SPECTROPHOTOMETRIC.ACID..METH., 
 -FLUORIDE..DISSOLVED..MG.L.AS.F., -ALKALINITY.FILTERED.SAMPLE.AS.CACO3..MG.L, -X, -NA.)
RioBCU<- RioBCU%>% select(-Day, -Start.Time)
names(RioBCU) <- c("Year", "Total", "Month", "MonthlyFlowAvg", "Month_Year", "H2OTemp_Centigrade", 
"SpConductance", "O2Dissolved", "H2OCLTotal", "SulfateTotal", "pH", "Inst_FlowStream", 
 "Alkalinity", "CaDissolved", "MgDissolved", "NaDissolved", "KDissolved", "SilicaDissolved", "Hardness" )
RioBCU<- RioBCU %>% select(-O2Dissolved, -Alkalinity)
RioBCU<- na.omit(RioBCU)
```

  Before joining the two datasets I created a new column for both datasets which combined the Month and Year, "MY", and then conducted an inner_join, because I only wanted to include the month-year combinations that were present in both datasets. I then removed some columns which only included a few variables and were mostly NAs, as I thought there wouldn't be enough data present to draw any conclusions from them. I then renamed the remaining variables, so they would have easier labels. I then ran na.omit to remove any rows that still contained NAs, as I only wanted data which contained every measurement. Dropping the NAs could potentially be problematic, because you don't have as much data to draw conclusions from, and could create a bias within your calculations. Depending on the dataset, an NA may have meaning and signify something as well.

3. Create summary statistics

    - Use *all six* core `dplyr` functions (filter, select, arrange, group_by, mutate, summarize) to manipulate and explore your dataset. For mutate, create a  new variable that is a function of at least one other variable, preferably using a dplyr vector function (see dplyr cheatsheet). It's fine to use the `_if`, `_at`, `_all` versions of mutate/summarize instead (indeed, it is encouraged if you have lots of variables)
    
    - Create summary statistics (mean, sd, var, n, quantile, min, max, n_distinct, cor, etc) for each of your numeric variables overall and after grouping by one of your categorical variables (either together or one-at-a-time; if you have two categorical variables, try to include at least one statistic based on a grouping of two categorical variables simultaneously). If you do not have any categorical variables, create one using mutate to satisfy the requirements above. Ideally, you will find a way to show these summary statistics in an easy-to-read table (e.g., by reshaping). If you have lots of numeric variables, or your categorical variables have too many categories, just pick a few (either numeric variables or categories of a categorical variable) and summarize based on those. It would be a good idea to show a correlation matrix for your numeric variables!
 
```{r}
library(dplyr)
colnames(RioBCU)[colnames(RioBCU)=="Inst_FlowStream"]<-"Instantaneous Stream Flow (CFS)"
RioBCU <- RioBCU %>% mutate(`%YearlyFlowAvg` = ((MonthlyFlowAvg/Total)*100))
RioBCU %>% select(Year, Month, MonthlyFlowAvg) %>% filter(MonthlyFlowAvg == max(MonthlyFlowAvg))
RioBCU %>% select(Year, Month, MonthlyFlowAvg) %>% filter(MonthlyFlowAvg == min(MonthlyFlowAvg))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, MonthlyFlowAvg) %>% filter(MonthlyFlowAvg == max(MonthlyFlowAvg))%>% arrange(desc(MonthlyFlowAvg)))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, MonthlyFlowAvg) %>% filter(MonthlyFlowAvg == min(MonthlyFlowAvg))%>% arrange(MonthlyFlowAvg))
RioBCU %>% select(Year, Month, H2OTemp_Centigrade) %>% filter(H2OTemp_Centigrade == max(H2OTemp_Centigrade))
RioBCU %>% select(Year, Month, H2OTemp_Centigrade) %>% filter(H2OTemp_Centigrade == min(H2OTemp_Centigrade))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, H2OTemp_Centigrade) %>% filter(H2OTemp_Centigrade == max(H2OTemp_Centigrade))%>% arrange(desc(H2OTemp_Centigrade)))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, H2OTemp_Centigrade) %>% filter(H2OTemp_Centigrade == min(H2OTemp_Centigrade))%>% arrange(H2OTemp_Centigrade))
RioBCU %>% select(Year, Month, SulfateTotal) %>% filter(SulfateTotal == max(SulfateTotal))
RioBCU %>% select(Year, Month, SulfateTotal) %>% filter(SulfateTotal == min(SulfateTotal))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, SulfateTotal) %>% filter(SulfateTotal == max(SulfateTotal))%>% arrange(desc(SulfateTotal)))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, SulfateTotal) %>% filter(SulfateTotal == min(SulfateTotal))%>% arrange(SulfateTotal))
RioBCU %>% select(Year, Month, pH) %>% filter(pH == max(pH))
RioBCU %>% select(Year, Month, pH) %>% filter(pH == min(pH))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, pH) %>% filter(pH == max(pH))%>% arrange(desc(pH)))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, pH) %>% filter(pH == min(pH))%>% arrange(pH))
RioBCU %>% select(Year, Month, `Instantaneous Stream Flow (CFS)`) %>% filter(`Instantaneous Stream Flow (CFS)` == max(`Instantaneous Stream Flow (CFS)`))
RioBCU %>% select(Year, Month, `Instantaneous Stream Flow (CFS)`) %>% filter(`Instantaneous Stream Flow (CFS)` == min(`Instantaneous Stream Flow (CFS)`))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, `Instantaneous Stream Flow (CFS)`) %>% filter(`Instantaneous Stream Flow (CFS)` == max(`Instantaneous Stream Flow (CFS)`))%>% arrange(desc(`Instantaneous Stream Flow (CFS)`)))
head(RioBCU %>% group_by(Year) %>% select(Year, Month, `Instantaneous Stream Flow (CFS)`) %>% filter(`Instantaneous Stream Flow (CFS)` == min(`Instantaneous Stream Flow (CFS)`))%>% arrange(`Instantaneous Stream Flow (CFS)`))
RioBCU %>% select(Year, Total) %>% filter(Total == max(Total)) %>% distinct()
RioBCU %>% select(Year, Total) %>% filter(Total == min(Total))%>% distinct()
RioBCU  %>% summarize_at(c(meanTFA="Total", meanMFA="MonthlyFlowAvg", meanWT="H2OTemp_Centigrade", meanSulfate="SulfateTotal",meanpH="pH", meanIFS="Instantaneous Stream Flow (CFS)"), mean, na.rm=T) 
head(RioBCU %>% group_by(Year) %>% summarize_at(c( meanMFA="MonthlyFlowAvg", meanWT="H2OTemp_Centigrade", meanSulfate="SulfateTotal",meanpH="pH", meanIFS="Instantaneous Stream Flow (CFS)"), mean, na.rm=T) )
RioBCU  %>% summarize_at(c(sdTFA="Total", sdMFA="MonthlyFlowAvg", sdWT="H2OTemp_Centigrade", sdSulfate="SulfateTotal",sdpH="pH", sdIFS="Instantaneous Stream Flow (CFS)"), sd, na.rm=T) 
head(RioBCU %>% group_by(Year) %>% summarize_at(c( sdMFA="MonthlyFlowAvg", sdWT="H2OTemp_Centigrade", sdSulfate="SulfateTotal",sdpH="pH", sdIFS="Instantaneous Stream Flow (CFS)"), sd, na.rm=T) )
RioBCU %>% 
  summarise(`25%`=quantile(MonthlyFlowAvg, probs=0.25),
            `50%`=quantile(MonthlyFlowAvg, probs=0.5),
            `75%`=quantile(MonthlyFlowAvg, probs=0.75))
head(RioBCU  %>% group_by(Year) %>%
  summarise(`25%`=quantile(MonthlyFlowAvg, probs=0.25),
            `50%`=quantile(MonthlyFlowAvg, probs=0.5),
            `75%`=quantile(MonthlyFlowAvg, probs=0.75)))
RioBCU %>% 
  summarise(`25%`=quantile(H2OTemp_Centigrade, probs=0.25),
            `50%`=quantile(H2OTemp_Centigrade, probs=0.5),
            `75%`=quantile(H2OTemp_Centigrade, probs=0.75))
head(RioBCU %>% group_by(Year) %>%
summarise(`25%`=quantile(H2OTemp_Centigrade, probs=0.25),
          `50%`=quantile(H2OTemp_Centigrade, probs=0.5),
          `75%`=quantile(H2OTemp_Centigrade, probs=0.75)))
RioBCU %>% 
  summarise(`25%`=quantile(SulfateTotal, probs=0.25),
            `50%`=quantile(SulfateTotal, probs=0.5),
            `75%`=quantile(SulfateTotal, probs=0.75))
head(RioBCU %>% group_by(Year) %>%
  summarise(`25%`=quantile(SulfateTotal, probs=0.25),
            `50%`=quantile(SulfateTotal, probs=0.5),
            `75%`=quantile(SulfateTotal, probs=0.75)))
RioBCU %>% 
  summarise(`25%`=quantile(pH, probs=0.25),
            `50%`=quantile(pH, probs=0.5),
            `75%`=quantile(pH, probs=0.75))
head(RioBCU %>% group_by(Year) %>%
  summarise(`25%`=quantile(pH, probs=0.25),
            `50%`=quantile(pH, probs=0.5),
            `75%`=quantile(pH, probs=0.75)))
RioBCU %>% 
  summarise(`25%`=quantile(`Instantaneous Stream Flow (CFS)`, probs=0.25),
            `50%`=quantile(`Instantaneous Stream Flow (CFS)`, probs=0.5),
            `75%`=quantile(`Instantaneous Stream Flow (CFS)`, probs=0.75))
head(RioBCU %>% group_by(Year) %>%
  summarise(`25%`=quantile(`Instantaneous Stream Flow (CFS)`, probs=0.25),
            `50%`=quantile(`Instantaneous Stream Flow (CFS)`, probs=0.5),
            `75%`=quantile(`Instantaneous Stream Flow (CFS)`, probs=0.75)))
head(RioBCU %>% select_at(vars(MonthlyFlowAvg,H2OTemp_Centigrade,SpConductance,H2OCLTotal,
                          SulfateTotal,pH,`Instantaneous Stream Flow (CFS)`,CaDissolved,MgDissolved,
                          NaDissolved,KDissolved,SilicaDissolved,Hardness))%>% cor%>%t)
head(RioBCU %>% select_at(vars(MonthlyFlowAvg,H2OTemp_Centigrade,
                          SulfateTotal,pH,`Instantaneous Stream Flow (CFS)`))%>% cor%>%t)
```
 
    I utilized mutate in order to create a new variable entitled "%YearlyFlowAvg", which shows what percentage of the total Average Flow (ac-ft/year) occurred in each month, by dividing MonthlyFlowAvg by Total and multiplying by 100. I then found the maximum and minimum values for the whole dataset, as well as grouped by year, for the variables 'MonthlyFlowAvg', 'H2OTemp_Centigrade', 'SulfateTotal', 'pH', `Instantaneous Stream Flow (CFS)`, and 'Total' using select, filter, and group_by. I then found the mean and standard deviation of these 6 variables using the summarize_at function, I also found the mean and standard deviation grouped by Year, using group_by. I then used summarize to find the quantiles of 'MonthlyFlowAvg', 'H2OTemp_Centigrade', 'SulfateTotal', 'pH', and `Instantaneous Stream Flow (CFS)`, I also found the quantiles of these 5 variables grouped by Year. Finally I created two correlation matrices using select_at and cor, the first displays all numeric variables in the dataset, and the second is a correlation matrix of the 5 variables I utilized while data wrangling.
 
4. Make visualizations

    - If you have 5 variables (the minimum), with 2 numeric variables (the minimum), create at least two effective plots with ggplot that illustrate some of the more interesting findings that your descriptive statistics have turned up.
    - Each plot should have at least three variables mapped to separate aesthetics (if correlation heatmap, etc, fine to do the same "variable" on both axes)
    - At least one plot should include `stat="summary"`
    - Each plot should include a supporting paragraph describing the relationships that are being visualized and any notable trends that are apparent
        - It is fine to include more, but limit yourself to 4. Plots should avoid being redundant! Four bad plots will get a lower grade than two good plots, all else being equal.
    - If doing a 3D plot (not encouraged, but sometimes useful), it's fine to use plotly for one plot (make the other(s) in ggplot).
    
```{r}
z = RioBCU %>% select_at(vars(MonthlyFlowAvg,H2OTemp_Centigrade,SpConductance,H2OCLTotal,
                              SulfateTotal,pH,`Instantaneous Stream Flow (CFS)`,CaDissolved,MgDissolved,
                              NaDissolved,KDissolved,SilicaDissolved,Hardness))%>% cor%>%t
z[lower.tri(z,diag=TRUE)]=NA
z=as.data.frame(as.table(z))
z=z[order(-abs(z$Freq)),]
tidycor<-RioBCU %>% select_at(vars(MonthlyFlowAvg,H2OTemp_Centigrade,SpConductance,H2OCLTotal,
                                   SulfateTotal,pH,`Instantaneous Stream Flow (CFS)`,CaDissolved,MgDissolved,
                                   NaDissolved,KDissolved,SilicaDissolved,Hardness))%>% cor%>%t %>%as.data.frame%>%
  rownames_to_column%>%
  pivot_longer(-1,names_to="Variables",values_to="correlation")
tidycor%>%ggplot(aes(rowname,Variables,fill=correlation))+
  geom_tile()+
  scale_fill_gradient2(low="blue",mid="white",high="red")+
  geom_text(aes(label=round(correlation,2)),color = "black", size = 2)+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  coord_fixed()

ggplot(data=RioBCU, aes(x=Month, y=MonthlyFlowAvg, fill=`Instantaneous Stream Flow (CFS)`))+ 
  geom_bar(stat="summary",fun.y="mean") +facet_wrap(~Year) +theme_dark()+ggtitle("Rio Grande Average & Instantaneous Stream Flows")+
  ylab("Monthly Average Stream Flow (ac-ft/mo)") + xlab("Month") +
  theme(axis.text.x = element_text(angle=90,hjust = 1, size=6)) +scale_y_continuous(labels = comma) + 
  scale_x_discrete(limits=month.abb)+
theme(axis.text.y = element_text(size=10, angle = 45))+scale_fill_gradient(low="blue", high="red") +geom_errorbar(aes(ymin=MonthlyFlowAvg-sd(MonthlyFlowAvg), ymax=MonthlyFlowAvg+sd(MonthlyFlowAvg)), width=.3,
                position=position_dodge(.5))
ggplot(data=RioBCU, aes(x=Month, y=NaDissolved, color= SpConductance))+
  geom_point() +facet_wrap(~Year) + theme_dark()+
  ggtitle("Dissolved Sodium & Specific Conductance of the Rio Grande by Month") +
  ylab("Dissolved Sodium (mg/L)") + theme(axis.text.x = element_text(angle=90,hjust = 1, size=6)) +
  scale_x_discrete(limits=month.abb)+
  theme(axis.text.y = element_text(size=10, angle = 45)) + scale_y_continuous(breaks=round(seq(min(RioBCU$NaDissolved), max(RioBCU$NaDissolved),by=30),1))+
  labs(color = "Specific Conductance (UMHOS/cm @25Â°C)") +scale_color_gradientn(colours = rainbow(3)) + geom_errorbar(aes(ymin=NaDissolved-sd(NaDissolved), ymax=NaDissolved+sd(NaDissolved)),width=.3,
                position=position_dodge(.5))
```
    
    I first created a correlation heatmap, with strong correlations as red, and weak as blue, a 0 correlation is displayed as white. I used this heatmap to determine correlation between all of my numeric variables, the strongest correlation was between Specific Conductance and Dissolved Sodium, which makes sense considering ions like Sodium as necessary for conductance in water. Other dissolved chemicals also displayed a correlation with the Specific Conductance. There was also a correlation between Monthly Flow Average and Instantaneous Stream Flow, as well as Hardness and Dissolved Calcium. The most negative correlation was between Dissolved Silica and the Chloride Content in Water. 
    
    My first graph displays the Average Stream Flow (ac-ft/mo), by month, and the bar colors display the Instantaneous Stream Flow (CFS), and is facet wrapped by Year. The graph displays the positive correlation between Instantaneous Stream Flow and the Monthly Average Flow, it also suggests that the Average Flows and Instantaneous Stream Flow are generally higher in the Spring and Fall, and lower in the Summer, although more data would have to be utilized to determine whether there was a true correlation between Average Stream Flow and the time of the year.
    
    My second graph displays the Dissolved Sodium (mg/L), by month, and the point colors display the Specific Conducatance (UMHOS/cm) of the water, it is facet wrapped by Year. The graph displays a positive correlation between Dissolved Sodium and the Specific Conductance. Dissolved Sodium levels do not appear to differ based on times of year, but rather vary more widely by year, suggesting a change in environmental factors, its interesting because the highest Dissolved Sodium Correlation appear to occur the same year as low Monthly Flow Averages (1988) and appear to decrease when flow levels increase again (1992). This makes sense as elements or chemicals in water increase in concentration when there is a smaller volume of water, although it is not represented in the correlation heatmap, so is like consequential.
    
5. Perform k-means/PAM clustering or PCA on (at least) your numeric variables.

    - Include all steps as we discuss in class, including a visualization.

    - If you don't have at least 3 numeric variables, or you want to cluster based on categorical variables too, convert them to factors in R, generate Gower's dissimilarity matrix on the data, and do PAM clustering on the dissimilarities.
    
    - Show how you chose the final number of clusters/principal components 
    
    - Interpret the final clusters/principal components 

```{r}
RioBCUN<- RioBCU%>% select(-Year, -Total, -Month, -Month_Year, -`%YearlyFlowAvg`)
Rio_nums<-RioBCUN%>% select_if(is.numeric)%>%scale
Rio_pca<- princomp(Rio_nums)
eigval<- Rio_pca$sdev^2
varprop=round(eigval/sum(eigval),2)
ggplot()+geom_bar(aes(y=varprop,x=1:13),stat = "identity")+xlab("")+geom_path(aes(y=varprop,x=1:13))+
  geom_text(aes(x=1:13, y=varprop,label=round(varprop,2)),vjust=1,col="white",size=5)+
  scale_y_continuous(breaks=seq(0,.6,.2),labels=scales::percent)+
  scale_x_continuous(breaks=1:10)
round(cumsum(eigval)/sum(eigval),2)
eigval
eigen(cor(Rio_nums))
Riodf<-data.frame(PC1=Rio_pca$scores[,1], PC2=Rio_pca$scores[,2])
ggplot(Riodf,aes(PC1, PC2))+geom_point()
Name<-RioBCU$Month_Year
Rio_pca$scores%>%as.data.frame%>%cbind(Name,.)%>% top_n(3, Comp.1)
Rio_pca$scores%>%as.data.frame%>%cbind(Name,.)%>% top_n(3,wt=desc(Comp.1))
Rio_pca$scores%>%as.data.frame%>%cbind(Name,.)%>% top_n(3, Comp.2)
Rio_pca$scores%>%as.data.frame%>%cbind(Name,.)%>% top_n(3,wt=desc(Comp.2))
head(RioBCU%>%filter(Month_Year%in%c("Oct - 1983", "Nov - 1991", "Oct - 1992")))
head(RioBCU%>%filter(Month_Year%in%c("Feb - 1988", "Mar - 1988", "Aug - 1988")))
head(RioBCU%>%filter(Month_Year%in%c("Sep - 1991", "Oct - 1991", "May - 1994")))
head(RioBCU%>%filter(Month_Year%in%c("Feb - 1983", "Jan - 1984", "Jan - 1985")))
Rio_pca$loadings[1:13,1:2]%>%as.data.frame%>%rownames_to_column%>%
  ggplot()+geom_hline(aes(yintercept=0),lty=2)+
  geom_vline(aes(xintercept=0),lty=2)+ylab("PC2")+xlab("PC1")+
  geom_segment(aes(x=0,y=0,xend=Comp.1,yend=Comp.2),arrow=arrow(),col="red")+
  geom_label_repel(aes(x=Comp.1*1.1,y=Comp.2*1.1,label=rowname), point.padding = T)
install.packages("FactoMineR", repos = "http://cran.us.r-project.org")
library(FactoMineR)
install.packages("factoextra", repos = "http://cran.us.r-project.org")
library(factoextra)
RioBCU$Year<-as.factor(RioBCU$Year)
fviz_pca_biplot(Rio_pca, col.ind=RioBCU$Year)+coord_fixed()
```
  
    PAM clustering was utilized in order to determine any correlations between the numeric variables within the dataset and group them into their correlations. According to the results conducted by PAM clustering, Instantaneous Stream Flow (CFS) and Monthly Avg Stream Flow (ac-ft/mo) are grouped together, along with Water Temperation (Centigrade). Dissolved Silica (mg/L) and pH are grouped together, and Specfic Conductance (UMHOS/cm), Dissolved Sodium, Calcium, Potassium, Sulfate, Magnesium, and Chloride (mg/L), as wel as Hardness, were grouped together. It appears the late 80's (1988-1990) are clustered more closely to the Dissolved Chemicals, and are mainly separated along dimension 1 from the early 80's and 90's (1982-1983 and 1991-1993). There is not as clear of a clustering across dimension2, although there are some clusters that appear above or below the line, 1985 and 1983 appear more closely clustered below dimension2, while 1994 and 1989 appear clustered on the dimension2 line. 
    
- For every step, you should document what your code does (in words) and what you see in the data.     
    
    
### Rubric

Prerequisite: Finding appropriate data from at least two sources per the instructions above: Failure to do this will result in a 0! You will submit a .Rmd file and a knitted document (pdf).

#### 0. Introduction (4  pts)

- Write a narrative introductory paragraph (or two) describing the datasets you have chosen, the variables they contain, how they were acquired, and why they are interesting to you. Expand on potential associations you may expect, if any.

#### 1. Tidying: Rearranging Wide/Long (8 pts)

- Tidy the datasets (using the `tidyr` functions `pivot_longer`/`gather` and/or `pivot_wider`/`spread`) 
- If you data sets are already tidy, untidy them, retidy them.
- Document the process (describe in words what was done)
    
#### 2. Joining/Merging (8 pts)

- Join your datasets into one using a `dplyr` join function
- If you have multiple observations on the joining variable in either dataset, fix this by collapsing via summarize
- Discuss the process in words, including why you chose the join you did
- Discuss which cases were dropped, if any, and potential problems with this

#### 3. Wrangling (40 pts)

- Use all six core `dplyr` functions in the service of generating summary statistics (18 pts)
    - Use mutate to generate a variable that is a function of at least one other variable

- Compute at least 10 different summary statistics using summarize and summarize with group_by (18 pts)
    - At least 2 of these should group by a categorical variable. Create one by dichotomizing a numeric if necessary
    - If applicable, at least 1 of these 5 should group by two categorical variables
    - Strongly encouraged to create a correlation matrix with `cor()` on your numeric variables

- Summarize/discuss all results in no more than two paragraphs (4 pts)


#### 4. Visualizing (30 pts)

- Create two effective, polished plots with ggplot

    - Each plot should map 3+ variables to aesthetics 
    - Each plot should have a title and clean labeling for all mappings
    - Change at least one default theme element and color for at least one mapping per plot
    - For at least one plot, add more tick marks (x, y, or both) than are given by default
    - For at least one plot, use the stat="summary" function
    - Supporting paragraph or two (for each plot) describing the relationships/trends that are apparent
    
#### 5. Dimensionality Reduction (20 pts) 

- Either k-means/PAM clustering or PCA (inclusive "or") should be performed on at least three numeric variables in your dataset

    - All relevant steps discussed in class should be included/reported
    - A visualization of the clusters or the first few principal components (using ggplot2)
    - Supporting paragraph or two describing results found 
    
##Where do I find data?
OK, brace yourself!
You can choose ANY datasets you want that meet the above criteria for variables and observations. Iâ€™m just sitting here but off the top of my head, if you are into amusement parks, you could look at amusement-park variables, including visits, and how they are impacted by weather. If you are interested in Game of Thrones, you could look at how the frequency of mentions of character names (plus other character variables) and the frequency of baby names in the US...You could even take your old Biostats data and merge in new data (e.g., based on a Google forms timestamp).
You can make it as serious as you want, or not, but keep in mind that you will be incorporating this project into a portfolio webpage for your final in this course, so choose something that really reflects who you are, or something that you feel will advance you in the direction you hope to move career-wise, or something that you think is really neat. On the flip side, regardless of what you pick, you will be performing all the same tasks, so it doesnâ€™t end up being that big of a deal.
If you are totally clueless and have no direction at all, log into the server and type

```{R eval=F}
data(package = .packages(all.available = TRUE))
```
This will print out a list of ALL datasets in ALL packages installed on the server (a ton)! Scroll until your eyes bleed! Actually, do not scroll that much... To start with something more manageable, just run the command on your own computer, or just run data() to bring up the datasets in your current environment. To read more about a dataset, do
?packagename::datasetname .
If it is easier for you, and in case you donâ€™t have many packages installed, a list of R datasets from a few common packages (also downloadable in CSV format) is given at the following website: https://vincentarelbundock.github.io/Rdatasets/datasets.html (https://vincentarelbundock.github.io/Rdatasets/datasets.html)
A good package to download for fun/relevant data is fivethiryeight . Just run install.packages("fivethirtyeight"), load the packages with library(fivethirtyeight) , r
and then scroll down to view the datasets. Here is an online list of all 127 datasets (with links to the 538 articles). Lots of sports, politics, current events, etc.
If you have already started to specialize (e.g., ecology, epidemiology) you might look at discipline-specific R packages (vegan, epi, respectively). We will be using some tools from these packages later in the course, but they come with lots of data too, which you can explore according to the directions above
However, you emphatically DO NOT have to use datasets available via R packages! In fact, I would much prefer it if you found the data from completely separate sources and brought them together (a much more realistic experience in the real world)! You can even reuse data from your SDS328M project, provided it shares a variable in common with other data which allows you to merge the two together (e.g., if you still had the timestamp, you could look up the weather that day: https://www.wunderground.com/history/ (https://www.wunderground.com/history/)). If you work in a research lab or have access to old data, you could potentially merge it with new data from your lab!
Here is a curated list of interesting datasets (read-only spreadsheet format): https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/e (https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/
Here is another great compilation of datasets: https://github.com/rfordatascience/tidytuesday (https://github.com/rfordatascience/tidytuesday)
Here is the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/index.php (https://archive.ics.uci.edu/ml/index.php)
See also https://en.wikipedia.org/wiki/List_of_datasets_for_machine- learning_research#Biological_data (https://en.wikipedia.org/wiki/List_of_datasets_for_machine- learning_research#Biological_data)
Here is another good general place to look: https://www.kaggle.com/datasets (https://www.kaggle.com/datasets)
To help narrow your search down or to see interesting variable ideas, check out https://www.tylervigen.com/spurious-correlations (https://www.tylervigen.com/spurious- correlations). This is the spurious correlations website, and it is fun, but if you look at the bottom of each plot you will see sources for the data. This is a good place to find very general data (or at least get a sense of where you can scrape data together from)!
If you are interested in medical data, check out www.countyhealthrankings.org
If you are interested in scraping UT data, they make loads of data public (e.g., beyond just professor CVs and syllabi). Check out all the data that is available in the statistical handbooks: https://reports.utexas.edu/statistical-handbook (https://reports.utexas.edu/statistical-handbook)
           Broader data sources:
file:///Users/Jade/Desktop/UT%20Year%203/rtest.html Page 27 of 28
un
dit edit)
Project 1: Exploratory Data Analysis 10/21/19, 9(32 AM
Data.gov (www.data.gov) 186,000+ datasets!
Social Explorer (Social%20Explorer) is a nice interface to Census and American Community Survey data (more user-friendly than the government sites). May need to sign up for a free trial.
U.S. Bureau of Labor Statistics (www.bls.gov)
U.S. Census Bureau (www.census.gov)
Gapminder (www.gapminder.org/data), data about the world.


...
